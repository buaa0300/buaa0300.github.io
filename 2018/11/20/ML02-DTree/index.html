<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="机器学习中的树模型"><meta name="keywords" content="ML,决策树"><meta name="author" content="南华coder"><meta name="copyright" content="南华coder"><title>机器学习中的树模型 | 南华coder的空间</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#一、开篇"><span class="toc-number">1.</span> <span class="toc-text">一、开篇</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1、概述"><span class="toc-number">1.1.</span> <span class="toc-text">1、概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2、决策树和集成学习"><span class="toc-number">1.2.</span> <span class="toc-text">2、决策树和集成学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#二、决策树"><span class="toc-number">2.</span> <span class="toc-text">二、决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1、决策树中需关注问题"><span class="toc-number">2.1.</span> <span class="toc-text">1、决策树中需关注问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2、如何构建一棵树"><span class="toc-number">2.2.</span> <span class="toc-text">2、如何构建一棵树</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1、ID3"><span class="toc-number">2.2.1.</span> <span class="toc-text">1、ID3</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1、基础概念"><span class="toc-number">2.2.2.</span> <span class="toc-text">1-1、基础概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2、不足"><span class="toc-number">2.2.3.</span> <span class="toc-text">1-2、不足</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2、C4-5"><span class="toc-number">2.2.4.</span> <span class="toc-text">2、C4.5</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1、基础概念"><span class="toc-number">2.2.5.</span> <span class="toc-text">2-1、基础概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2、不足"><span class="toc-number">2.2.6.</span> <span class="toc-text">2-2、不足</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3、CART"><span class="toc-number">2.2.7.</span> <span class="toc-text">3、CART</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1、基础概念"><span class="toc-number">2.2.8.</span> <span class="toc-text">3-1、基础概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2、特点"><span class="toc-number">2.2.9.</span> <span class="toc-text">3-2、特点</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3、树的剪枝"><span class="toc-number">2.3.</span> <span class="toc-text">3、树的剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1、预剪枝"><span class="toc-number">2.3.1.</span> <span class="toc-text">3-1、预剪枝</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2、后剪枝"><span class="toc-number">2.3.2.</span> <span class="toc-text">3-2、后剪枝</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三、梯度提升树-和-随机森林"><span class="toc-number">3.</span> <span class="toc-text">三、梯度提升树 和 随机森林</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">南华coder</div><div class="author-info__description text-center">于无声处听惊雷,于无色处见繁花</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">36</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">49</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">11</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">南华coder的空间</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">机器学习中的树模型</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-20</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="一、开篇"><a href="#一、开篇" class="headerlink" title="一、开篇"></a>一、开篇</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><ul>
<li>在数据结构中，有树这种结构；在机器学习中，有决策树；咋一看，感觉是一回事，其实不然；</li>
<li><p>数据结构的树关注的事查找，插入，删除的效率，二叉树，平衡二叉树都是为了解决这些效率而产生的；</p>
</li>
<li><p>机器学习中的决策树关注的是，如何找到最佳分解节点，<strong>ID3算法</strong>(按最大信息增益划分)，<strong>C4.5</strong>（按信息增益比划分），<strong>CART算法</strong>（按最小基尼指数划分）都是为了达到最佳分裂的效果。</p>
</li>
<li>最大信息增益会倾向于<strong>可取值较多</strong>的特征，最大信息增益比会倾向于<strong>可取值较少的特征</strong>。</li>
</ul>
<h4 id="2、决策树和集成学习"><a href="#2、决策树和集成学习" class="headerlink" title="2、决策树和集成学习"></a>2、决策树和集成学习</h4><ul>
<li>决策树模型很简单，简单到即使你不懂机器学习，也能很快了解他，可能有人会说，这么简单的模型有什么用。哈哈，数学的伟大，在于复杂的问题简单化，深度学习的强大在于利用将简单的神经网络不断加深；决策树虽然简单，但是很多个决策树在一起，就足够让很多浅层机器学习算法忘而却步。</li>
<li><p>这个将许多决策树整合一起的方式就是集成学习，在集成学习中，决策树是个体学习器，当然个体学习器可以是别的弱学习器。</p>
</li>
<li><p>而根据个体学习器生成方式的不同，目前集成学习方法大致可分为两大类，即个体学习器间存在<strong>强依赖关系</strong>、必须串行生成的序列化方法，以及个体学习器间<strong>不存在强依赖关系</strong>、可同时生成的并行化方法；前者的代表是<strong>Boosting </strong> 和 <strong>梯度提升树</strong>(GBDT)，后者的代表是和<strong>Bagging</strong>和 <strong>随机森林</strong>（Random Forest）</p>
</li>
</ul>
<h3 id="二、决策树"><a href="#二、决策树" class="headerlink" title="二、决策树"></a>二、决策树</h3><h4 id="1、决策树中需关注问题"><a href="#1、决策树中需关注问题" class="headerlink" title="1、决策树中需关注问题"></a>1、决策树中需关注问题</h4><ul>
<li><p>一棵树是如何构建的？建树过程中，树分裂节点时，如何选出最优的属性作为分裂节点。</p>
</li>
<li><p>如何用树的减枝来避免过拟合问题。</p>
</li>
<li><p>对于含有空值的数据，如何构建树。</p>
</li>
<li><p>构建树可能存在的问题，过拟合问题，如何解决</p>
</li>
</ul>
<h4 id="2、如何构建一棵树"><a href="#2、如何构建一棵树" class="headerlink" title="2、如何构建一棵树"></a>2、如何构建一棵树</h4><p>简言之：<strong>选择最优划分属性作为分裂结点，使得分支结点中所包含的样本尽可能属于同一类</strong>。树的生成算法有三种：ID3、C4.5和CART.</p>
<h5 id="1、ID3"><a href="#1、ID3" class="headerlink" title="1、ID3"></a>1、ID3</h5><p>选择<strong>最大信息增益(Information Gain)</strong>规则去寻找最优分裂节点</p>
<h5 id="1-1、基础概念"><a href="#1-1、基础概念" class="headerlink" title="1-1、基础概念"></a>1-1、基础概念</h5><ul>
<li><p>信息熵(Information entropy)：表示随机变量不确定性的度量；<strong>熵越大，随机变量的不确定性就越大</strong>；(我们希望分类后的结果熵越小越好)。</p>
</li>
<li><p>信息增益(Information gain)：表示因特征X的信息而使得类Y信息不确定性减少的程度。(当然是越大越好）；<br>$$<br>g(D,A) = H(D) - H(D|A)<br>$$</p>
</li>
</ul>
<p>在划分过程中，找到信息增益最大的特征将样本根据此特征划分不同的结点中，新的结点中继续划分。</p>
<h5 id="1-2、不足"><a href="#1-2、不足" class="headerlink" title="1-2、不足"></a>1-2、不足</h5><ul>
<li>信息增益反应的是：给点条件后，不确定性减少的程度，<strong>特征取值越多，意味着确定性越高，也就是条件熵越小，信息增益越大。</strong>这就造成信息增益对<strong>可取值较多</strong>的属性有所偏好。</li>
<li>ID3只能处理离散型变量，只能处理分类任务</li>
<li>对样本特征缺失值比较敏感</li>
</ul>
<h5 id="2、C4-5"><a href="#2、C4-5" class="headerlink" title="2、C4.5"></a>2、C4.5</h5><p>选择<strong>最大信息增益比(Information Gain Ratio)</strong>规则去寻找最优分裂节点</p>
<h5 id="2-1、基础概念"><a href="#2-1、基础概念" class="headerlink" title="2-1、基础概念"></a>2-1、基础概念</h5><ul>
<li><strong>信息增益比(Information Gain Ratio)</strong>: 特征A对训练数据集D的信息增益比为其 信息增益与训练数据集D关于特征A的值的熵之比。<br>$$<br>g_R(D,A) =\frac{g(D,A)}{H_A(D)}<br>$$</li>
</ul>
<p>在划分过程中，找到信息增益比最大的特征将样本根据此特征划分不同的结点中，新的结点中继续划分。</p>
<h5 id="2-2、不足"><a href="#2-2、不足" class="headerlink" title="2-2、不足"></a>2-2、不足</h5><ul>
<li>C4.5 对ID3做了优化，使用信息增益比在一定程度上对取值较多的特征进行了惩罚，避免ID3出现的过拟合特性，提升了决策树的泛化能力。</li>
<li>C4.5 能处理连续型变量和离散型变量，但是只能处理分类任务。</li>
</ul>
<h5 id="3、CART"><a href="#3、CART" class="headerlink" title="3、CART"></a>3、CART</h5><p>选择<strong>最小基尼指数(Gini index)</strong>规则去寻找最优分裂节点</p>
<h5 id="3-1、基础概念"><a href="#3-1、基础概念" class="headerlink" title="3-1、基础概念"></a>3-1、基础概念</h5><ul>
<li>基尼指数：表示数据的纯度<br>$$<br>Cini(D) = 1 - \sum_{k=1}^n(\frac{|C_k|}{|D|})^2<br>$$</li>
</ul>
<p>利用基尼指数最小选择最优分裂点，采用二元切割法</p>
<h5 id="3-2、特点"><a href="#3-2、特点" class="headerlink" title="3-2、特点"></a>3-2、特点</h5><ul>
<li>不仅能处理分类任务，还能处理回归任务</li>
<li>不仅能处理离散型变量，还能处理连续型变量</li>
<li>能够处理样本特征数据缺失的情况</li>
</ul>
<h4 id="3、树的剪枝"><a href="#3、树的剪枝" class="headerlink" title="3、树的剪枝"></a>3、树的剪枝</h4><h5 id="3-1、预剪枝"><a href="#3-1、预剪枝" class="headerlink" title="3-1、预剪枝"></a>3-1、预剪枝</h5><p>阈值、</p>
<h5 id="3-2、后剪枝"><a href="#3-2、后剪枝" class="headerlink" title="3-2、后剪枝"></a>3-2、后剪枝</h5><h3 id="三、梯度提升树-和-随机森林"><a href="#三、梯度提升树-和-随机森林" class="headerlink" title="三、梯度提升树 和 随机森林"></a>三、梯度提升树 和 随机森林</h3><ul>
<li><p>梯度提升树，GBDT，特征组合的一把好手</p>
</li>
<li><p>随机森林 RF</p>
</li>
<li><p>基于决策树实现的GBDT、RF（随机森林）、Xgboost 和 lightGBM都在数据竞赛和工程中都大展身手。</p>
</li>
<li><p><a href="https://github.com/e-snail/understanding_machine_learning/tree/master/3_decision_tree" target="_blank" rel="noopener">https://github.com/e-snail/understanding_machine_learning/tree/master/3_decision_tree</a></p>
</li>
<li><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html" target="_blank" rel="noopener">决策树API</a></p>
</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">南华coder</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://buaa0300/nanhuacoder.com/2018/11/20/ML02-DTree/">http://buaa0300/nanhuacoder.com/2018/11/20/ML02-DTree/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://buaa0300/nanhuacoder.com">南华coder的空间</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ML/">ML</a><a class="post-meta__tags" href="/tags/决策树/">决策树</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/11/21/Recommend01/"><i class="fa fa-chevron-left">  </i><span>推荐迷雾(一):再见推荐系统</span></a></div><div class="next-post pull-right"><a href="/2018/11/17/ML01/"><span>机器学习杂谈</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By 南华coder</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>